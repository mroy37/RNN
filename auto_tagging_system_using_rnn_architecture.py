# -*- coding: utf-8 -*-
"""Auto Tagging System using RNN Architecture

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Y3zUwB7aJw4LfY92r8cGinvyCx0tnfHN

##importing libraries
"""

import zipfile
import ast
import pandas as pd
import re
pd.set_option('display.max_colwidth', 200)
import numpy as np
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
from sklearn import metrics
from sklearn.preprocessing import MultiLabelBinarizer

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Embedding, LSTM, Dense, BatchNormalization, Dropout
from tensorflow.keras.preprocessing.sequence import pad_sequences

import nltk
nltk.download('stopwords')
nltk.download('punkt')
stop_words = set(stopwords.words('english'))
from nltk.stem.snowball import SnowballStemmer
stemmer = SnowballStemmer("english")
from nltk.stem.wordnet import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from gensim.models import Word2Vec
from bs4 import BeautifulSoup

from google.colab import drive
drive.mount('/content/drive')

# Paths of the ZIP files
question_zip= '/content/drive/MyDrive/Questions.csv.zip'
ans_zip = '/content/drive/MyDrive/Answers.csv.zip'
tags_zip = '/content/drive/MyDrive/Tags.csv.zip'

# Extracted directory
extracted_dir_path = '/content/drive/MyDrive Auto Tag Prediction/'

'''# Unzipping questions
with zipfile.ZipFile(question_zip, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir_path)
# Unzipping answers
with zipfile.ZipFile(ans_zip, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir_path)
# Unzipping tags
with zipfile.ZipFile(tags_zip, 'r') as zip_ref:
    zip_ref.extractall(extracted_dir_path)'''

!unzip "/content/drive/MyDrive/Answers.csv.zip"
!unzip "/content/drive/MyDrive/Questions.csv.zip"
!unzip "/content/drive/MyDrive/Tags.csv.zip"

# Question
question = pd.read_csv('/content/Questions.csv', encoding='latin1')
# Answers
ans = pd.read_csv('/content/Answers.csv', encoding='latin1')
# Tags
tags = pd.read_csv('/content/Tags.csv', encoding='latin1')

question.head()

question['Body'][0]

ans.head()

tags.head(2)

"""##Data Preparation"""

tags['Tag'].nunique()

# remove "-" from the tags
tags['Tag'] = tags['Tag'].apply(lambda x:re.sub("-"," ",x))

# group tags Id wise
tags = tags.groupby('Id').apply(lambda x:x['Tag'].values).reset_index(name='tags')
tags.head()

# merge tags and questions
df = pd.merge(question,tags, how = 'inner', on = 'Id')
df = df[['Id','Body','tags']]
df.head(10)

df.shape

# Checking the occurence of the tags

freq = {}
for i in df['tags']:
    for j in i:
        if j in freq.keys():
            freq[j] = freq[j] +1
        else:
            freq[j] =1

# we can sort the dictionary in descending order
freq = dict(sorted(freq.items(), key = lambda x:x[1], reverse= True))

# Top 10 most frequent tags
top_10_tags = list(freq.keys())[:10]
print(top_10_tags)

# finding the queries associated with common tags

x =[]
y=[]

for i in range(len(df['tags'])):
    temp = []
    for j in df['tags'][i]:
        if j in top_10_tags:
            temp.append(j)
    if len(temp)>1:
        x.append(df['Body'][i])
        y.append(temp)

y[:5]

# We should combine the labels by space
y = [",".join([str(j) for j in i]) for i in y]

y[:5]

dframe = pd.DataFrame({'query':x, 'tags':y})

dframe.head()

dframe['query'][0]

"""## Text Cleaning & Preprocessing"""

def train_word2vec(tokenized_text):
    model = Word2Vec(sentences=tokenized_text, vector_size=100, window=5, min_count=1)
    return model

def word_to_vec(tokenized_text, model):
    vecs = []
    for tokens in tokenized_text:
        vec = []
        for token in tokens:
            if token in model.wv:
                vec.extend(model.wv[token])
        vecs.append(vec)
    return vecs

def data_clean(df):
    df['cleaned_text'] = df['query'].apply(lambda x: BeautifulSoup(x, "html.parser").get_text()) # To Remove HTML tags
    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words)) # To Remove Stop Words
    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: re.sub("[^a-zA-Z]", " ", x)) # To Remove any non alphabetic characters

    df['cleaned_text'] = df['cleaned_text'].str.lower() # To Lower all texts

    df['tokenized_text'] = df['cleaned_text'].apply(lambda x: word_tokenize(x)) # Tokenization

    df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [token.strip() for token in x])
    df['tokenized_text'] = df['tokenized_text'].apply(lambda x: [stemmer.stem(word) for word in x])

    model = train_word2vec(df['tokenized_text'])

    df['vectors'] = word_to_vec(df['tokenized_text'], model)

    return df[['vectors', 'tags']]

df = data_clean(dframe)

"""**Converting the labels using MultiLabelBinarization**"""

def multi_label_binarization(df):

    tags = df['tags'].str.split(',')

    mlb = MultiLabelBinarizer()

    binary_tags = mlb.fit_transform(tags)

    binary_tags_df = pd.DataFrame(binary_tags, columns=mlb.classes_)

    df = pd.concat([df, binary_tags_df], axis=1)

    df = df.drop(columns=['tags'])

    return df

final_df = multi_label_binarization(df)

final_df.to_csv('Final_DataFrame.csv', index= False)

final_df['vectors'][0]

max_seq_length = 1000
X_padded = pad_sequences(final_df['vectors'], maxlen=max_seq_length, dtype='float32', padding='post', truncating='post', value=0.0)

X_padded.shape

y = final_df.drop(columns=['vectors']).values

x_train,x_test,y_train,y_test=train_test_split(X_padded, y, test_size=0.2, random_state=9)

X_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
X_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

early_stopping = EarlyStopping(monitor = 'accuracy' , patience = 2  ,restore_best_weights = True )
model = Sequential()
model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2]), return_sequences=True))
model.add(Dropout(0.3))
model.add(LSTM(50))
model.add(Dropout(0.2))
model.add(Dense(10, activation='softmax'))


model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test),verbose=1,callbacks=[early_stopping])

y_pred = model.predict(X_test)
from sklearn.metrics import classification_report

# Convert the one-hot encoded labels back to original labels
y_pred = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

# Generate classification report
class_report = classification_report(y_true, y_pred)
print(class_report)